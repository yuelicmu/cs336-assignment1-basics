{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd04b31",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE) Tokenizer\n",
    "In the first part of the assignment, we will train and implement a byte-level byte-pair encoding (BPE)\n",
    "tokenizer [Sennrich et al., 2016, Wang et al., 2019]. In particular, we will represent arbitrary (Unicode)\n",
    "strings as a sequence of bytes and train our BPE tokenizer on this byte sequence. Later, we will use this\n",
    "tokenizer to encode text (a string) into tokens (a sequence of integers) for language modeling.\n",
    "\n",
    "## The Unicode Standard\n",
    "In Python, you can use the `ord()` function\n",
    "to convert a single Unicode character into its integer representation. The `chr()` function converts an integer\n",
    "Unicode code point into a string with the corresponding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0663a612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('牛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c43040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'牛'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(29275)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f13fee",
   "metadata": {},
   "source": [
    "- What Unicode character does chr(0) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c285ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a6a78",
   "metadata": {},
   "source": [
    "- How does this character’s string representation (__repr__()) differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b7a29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f0e31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(repr(chr(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3bcb9",
   "metadata": {},
   "source": [
    "- What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f91fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e831f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5bad1",
   "metadata": {},
   "source": [
    "## Unicode Encodings\n",
    "\n",
    "While the Unicode standard defines a mapping from characters to code points (integers), it’s impractical to\n",
    "train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around\n",
    "150K items) and sparse (since many characters are quite rare). Instead, we’ll use a Unicode encoding, which\n",
    "converts a Unicode character into a sequence of bytes. The Unicode standard itself defines three encodings:\n",
    "UTF-8, UTF-16, and UTF-32, with UTF-8 being the dominant encoding for the Internet (more than 98%\n",
    "of all webpages).\n",
    "\n",
    "To encode a Unicode string into UTF-8, we can use the `encode()` function in Python. To access the\n",
    "underlying byte values for a Python bytes object, we can iterate over it (e.g., call `list()`). Finally, we can\n",
    "use the `decode()` function to decode a UTF-8 byte string into a Unicode string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e0ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
      "13\n",
      "23\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "\n",
    "print(type(utf8_encoded))\n",
    "\n",
    "# Get the byte values for the encoded string (integers from 0 to 255).\n",
    "print(list(utf8_encoded))\n",
    "\n",
    "# One byte does not necessarily correspond to one Unicode character!\n",
    "print(len(test_string))\n",
    "print(len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220b8db",
   "metadata": {},
   "source": [
    "By converting our Unicode codepoints into a sequence of bytes (e.g., via the UTF-8 encoding), we\n",
    "are essentially taking a sequence of codepoints (integers in the range 0 to 154,997) and transforming it\n",
    "into a sequence of byte values (integers in the range 0 to 255). The 256-length byte vocabulary is much\n",
    "more manageable to deal with. When using byte-level tokenization, we do not need to worry about out-of-\n",
    "vocabulary tokens, since we know that any input text can be expressed as a sequence of integers from 0 to\n",
    "255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee7c2b",
   "metadata": {},
   "source": [
    "- Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87dc9677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d003422e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello\\xe7\\x89\\x9b'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello牛\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f31739",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe7 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello牛\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe7 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello牛\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7966bad",
   "metadata": {},
   "source": [
    "- Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6561fc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'牛'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(b\"\\xe7\\x89\\x9b\").decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d51f7b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 0-1: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\xe7\u001b[39;49;00m\u001b[38;5;130;43;01m\\x89\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode bytes in position 0-1: unexpected end of data"
     ]
    }
   ],
   "source": [
    "bytes(b\"\\xe7\\x89\").decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7dbe1",
   "metadata": {},
   "source": [
    "## Experimenting with BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcb913a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5f474",
   "metadata": {},
   "source": [
    "Implementation remarks:\n",
    "- Use `uv run pytest tests/test_train_bpe.py` for unit test of the tokenizer part. There are three tests:\n",
    "    - `test_train_bpe_speed`: pass if running time < 1.5 second on `corpus.en`;\n",
    "    - `test_train_bpe`: pass if the constructed vocab and values match the gpt2 reference;\n",
    "    - `test_train_bpe_special_tokens`: pass if b\"<|\" not merged in bytes?\n",
    "- Modify adapters.py which is desigend to be a wrapper around the actual function;\n",
    "- `pretokenization_example.py` as a (slow?) example of pre-tokenization implementation;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f9f3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ef8d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 98, 99, 231, 137, 155, 231, 137, 155]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"abc牛牛\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162490c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 124, 101, 110, 100, 111, 102, 116, 101, 120, 116, 124, 62]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"<|endoftext|>\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72352238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
